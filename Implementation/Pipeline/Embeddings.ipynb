{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ZsofiaK/masterthesis/blob/main/Implementation/Pipeline/Embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ETboycQ3Cts4"
      },
      "source": [
        "# DINOv2 embeddings\n",
        "\n",
        "This notebook generates and saves DINOv2 embeddings of pre-selected frames from a video dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mYW7FqsXCsqA",
        "outputId": "d79ab62a-9ddb-490b-e711-07e0b78e0ef9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Mount Drive.\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "_zHzpB0hGku7"
      },
      "outputs": [],
      "source": [
        "dataset_dir = 'Fish clips'\n",
        "\n",
        "dataset_name = 'fishClips'\n",
        "\n",
        "frame_selection_method = 'motionAbsdiff_10'\n",
        "\n",
        "DINO_model = 'dinov2_vitg14'\n",
        "\n",
        "feature_extraction = 'clf'\n",
        "\n",
        "image_size = 448    # Size to use when downsampling the frames (shorter side).\n",
        "\n",
        "patch_size = 14     # A characteristic of the DINOv2 model.\n",
        "\n",
        "embedding_method = f'{DINO_model}-{feature_extraction}'.replace('_', '-')\n",
        "\n",
        "immediate_copy = True   # If the embeddings should immediately be copied to Drive.\n",
        "\n",
        "skip_existing = True    # If existing embeddings should not be calculated again."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "6HfEmHKyaKFo"
      },
      "outputs": [],
      "source": [
        "# Specify directory to save embeddings\n",
        "import os\n",
        "drive_save_dir = f\"/content/drive/My Drive/UvA/M Thesis/Data/{dataset_dir}/Embeddings/{frame_selection_method}/{embedding_method}/{image_size}\"\n",
        "\n",
        "os.makedirs(drive_save_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LiyfRxO5IC04",
        "outputId": "dbc9f6bf-9ce3-4f87-aeda-00ffef6a9277"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://github.com/facebookresearch/dinov2/zipball/main\" to /root/.cache/torch/hub/main.zip\n",
            "/root/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/swiglu_ffn.py:51: UserWarning: xFormers is not available (SwiGLU)\n",
            "  warnings.warn(\"xFormers is not available (SwiGLU)\")\n",
            "/root/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:33: UserWarning: xFormers is not available (Attention)\n",
            "  warnings.warn(\"xFormers is not available (Attention)\")\n",
            "/root/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/block.py:40: UserWarning: xFormers is not available (Block)\n",
            "  warnings.warn(\"xFormers is not available (Block)\")\n",
            "Downloading: \"https://dl.fbaipublicfiles.com/dinov2/dinov2_vitg14/dinov2_vitg14_pretrain.pth\" to /root/.cache/torch/hub/checkpoints/dinov2_vitg14_pretrain.pth\n",
            "100%|██████████| 4.23G/4.23G [00:51<00:00, 87.8MB/s]\n"
          ]
        }
      ],
      "source": [
        "# DINOv2 version to use.\n",
        "import torch\n",
        "\n",
        "lvm = torch.hub.load('facebookresearch/dinov2', DINO_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "eCBbxZdzDEZe",
        "outputId": "0ce39de4-1cb3-4a64-f5ae-04f722557047"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/Fish clips'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# Copy dataset\n",
        "import shutil\n",
        "\n",
        "data_source = f\"/content/drive/My Drive/UvA/M Thesis/Data/{dataset_dir}\"\n",
        "data_dir = f\"/content/{dataset_dir}\"\n",
        "\n",
        "# Copy the folder to destination\n",
        "shutil.copytree(data_source, data_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "lsHsXz3_D5e9"
      },
      "outputs": [],
      "source": [
        "# Auxilliary functions.\n",
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "def calculate_new_dimensions(frame, image_size, patch_size):\n",
        "  '''\n",
        "  Calculates the downsampled dimensions of images.\n",
        "\n",
        "  :param: frame: the frame to downsample.\n",
        "  :param: image_size: the downsampled length of the smaller side of the frame.\n",
        "  :param: patch_size: the patch size of the LVM which will be used for embeddings.\n",
        "  '''\n",
        "\n",
        "  height, width, _ = frame.shape\n",
        "\n",
        "  # Calculate new image dimensions.\n",
        "  if width > height:\n",
        "    new_width = image_size\n",
        "    new_height = int(height * image_size / width)\n",
        "\n",
        "  else:\n",
        "    new_height = image_size\n",
        "    new_width = int(width * image_size / height)\n",
        "\n",
        "  # Ensure that both dimensions are multiples of the patch size.\n",
        "  if new_width % patch_size != 0:\n",
        "    new_width = (new_width // patch_size) * patch_size\n",
        "\n",
        "  if new_height % patch_size != 0:\n",
        "    new_height = (new_height // patch_size) * patch_size\n",
        "\n",
        "  return new_width, new_height\n",
        "\n",
        "def generate_default_embedding(frame, transform):\n",
        "  '''\n",
        "  Generates a default embedding of a frame.\n",
        "\n",
        "  :param: frame: the frame as a numpy array.\n",
        "  :param: transform: the torchvision transforms object with the necessary transformations.\n",
        "  :return: the embedding as a numpy array.\n",
        "  '''\n",
        "\n",
        "  img = transform(Image.fromarray(frame))[:3].unsqueeze(0)\n",
        "\n",
        "  with torch.no_grad():\n",
        "    # Note: lvm is the loaded large vision model to generate the embedding.\n",
        "    embedding = lvm(img)[0]\n",
        "\n",
        "  return embedding.squeeze().numpy()\n",
        "\n",
        "# Method dictionary for later easy of use.\n",
        "embedding_methods = {'clf' : generate_default_embedding}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "kEXCIyn2FK6y"
      },
      "outputs": [],
      "source": [
        "# Read frame selection table.\n",
        "import pandas as pd\n",
        "\n",
        "selection_table_name = f'{dataset_name}_{frame_selection_method}.csv'\n",
        "\n",
        "frames_df = pd.read_csv(f'/content/{dataset_dir}/Selected frames/{selection_table_name}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wUinXqCYxE_M",
        "outputId": "a4f74b94-ddfd-47a1-af5e-25cda1d9dbaa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of videos: 220\n",
            "Progress: 100.00%\n"
          ]
        }
      ],
      "source": [
        "import torchvision.transforms as T\n",
        "import cv2\n",
        "import numpy as np\n",
        "from IPython.display import clear_output\n",
        "\n",
        "# Select method of feature extraction.\n",
        "generate_embedding = embedding_methods[feature_extraction]\n",
        "\n",
        "save_dir = f'{data_dir}/Embeddings/{frame_selection_method}/{embedding_method}/{image_size}'\n",
        "os.makedirs(save_dir, exist_ok = True)\n",
        "\n",
        "nr_videos = len(frames_df.index)\n",
        "\n",
        "failed_frames = []\n",
        "\n",
        "transformation_set = False    # Marks if image transformation process has been set.\n",
        "\n",
        "for i, row in frames_df.iterrows():\n",
        "    # Show progress\n",
        "    clear_output(wait=True)\n",
        "    print(f'Number of videos: {nr_videos}')\n",
        "    print(f'Progress: {(i + 1 )/ nr_videos * 100:.2f}%')\n",
        "\n",
        "    video_file = row['video']\n",
        "    frame_indices = eval(row['frames'])\n",
        "\n",
        "    # Specify destination to save embedding\n",
        "    video_name = video_file.replace(\".mp4\", \"\")\n",
        "    save_path = f'{save_dir}/{video_name}.npy'\n",
        "\n",
        "    # Check if embedding already exists and skip if it does (optional).\n",
        "    if skip_existing:\n",
        "\n",
        "      # Skip if embedding already exists in Drive.\n",
        "      if os.path.exists(f'{drive_save_dir}/{video_name}.npy'):\n",
        "        continue\n",
        "\n",
        "      # Skip if already exists in runtime.\n",
        "      elif os.path.exists(save_path):\n",
        "\n",
        "        # Copy to Drive if immediate copy is enabled.\n",
        "        if immediate_copy:\n",
        "          shutil.copy(save_path, f'{drive_save_dir}/{video_name}.npy')\n",
        "\n",
        "        continue\n",
        "\n",
        "    video_path = f'/content/{dataset_dir}/Clips/{video_file}'\n",
        "\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "    embeddings = []\n",
        "\n",
        "    for frame_idx in frame_indices:\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n",
        "        ret, frame = cap.read()\n",
        "\n",
        "        if ret:\n",
        "          if not transformation_set:\n",
        "            new_width, new_height = calculate_new_dimensions(frame, image_size, patch_size)\n",
        "\n",
        "            # Set transformations to use\n",
        "            transform = T.Compose([\n",
        "                T.Resize((new_height, new_width)),\n",
        "                T.ToTensor()\n",
        "                ])\n",
        "\n",
        "            transformation_set = True\n",
        "\n",
        "          # Convert frame to RGB (from BGR)\n",
        "          frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "          embedding = generate_embedding(frame, transform)\n",
        "\n",
        "          embeddings.append(embedding)\n",
        "\n",
        "          # Clearing memory\n",
        "          frame = None\n",
        "          embedding = None\n",
        "\n",
        "        else:\n",
        "          failed_frames.append((video_file, frame_idx))\n",
        "          print('WARNING: failed to read frame.')\n",
        "\n",
        "    # Clearing memory.\n",
        "    cap.release()\n",
        "    cap = None\n",
        "\n",
        "    embeddings = np.concatenate(embeddings, axis=0)\n",
        "\n",
        "    # Save concatenated embeddings as a NumPy array\n",
        "    np.save(save_path, embeddings)\n",
        "\n",
        "    # Copy file to Drive (optional).\n",
        "    if immediate_copy:\n",
        "      shutil.copy(save_path, f'{drive_save_dir}/{video_name}.npy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JDsD0PcySK3n",
        "outputId": "bd13e3f5-7c97-487c-f31e-fa1a8432486c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Success! All frames read.\n"
          ]
        }
      ],
      "source": [
        "if len(failed_frames) > 0:\n",
        "  print('WARNING: some frames were not read.')\n",
        "\n",
        "else:\n",
        "  print('Success! All frames read.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zLS0xeM6ytgj",
        "outputId": "f9d67cc3-a076-460e-f6b7-ae8c44db7c9d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embeddings have already been copied to Drive.\n"
          ]
        }
      ],
      "source": [
        "# Copy embeddings to Drive if they have not been already.\n",
        "if not immediate_copy:\n",
        "  shutil.copytree(save_dir, drive_save_dir)\n",
        "\n",
        "else:\n",
        "  print('Embeddings have already been copied to Drive.')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPNzMoXCO5PT8pzBLtvYmLP",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}