{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ZsofiaK/masterthesis/blob/main/Implementation/Pipeline/Embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ETboycQ3Cts4"
      },
      "source": [
        "# DINOv2 embeddings\n",
        "\n",
        "This notebook generates and saves DINOv2 embeddings of pre-selected frames from a video dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mYW7FqsXCsqA",
        "outputId": "28792926-c9ef-4a47-949e-e7f60c264168"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Mount Drive.\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "_zHzpB0hGku7"
      },
      "outputs": [],
      "source": [
        "dataset_dir = 'AK fish'\n",
        "\n",
        "dataset_name = 'AK-fish'\n",
        "\n",
        "frame_selection_method = 'motionAbsdiff_10'\n",
        "\n",
        "DINO_model = 'dinov2_vits14'\n",
        "\n",
        "feature_extraction = 'clf'\n",
        "\n",
        "image_size = 448    # Size to use when downsampling the frames (shorter side).\n",
        "\n",
        "patch_size = 14     # A characteristic of the DINOv2 model.\n",
        "\n",
        "embedding_method = f'{DINO_model}-{feature_extraction}'.replace('_', '-')\n",
        "\n",
        "immediate_copy = True   # If the embeddings should immediately be copied to Drive.\n",
        "\n",
        "skip_existing = True    # If existing embeddings should not be calculated again."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "6HfEmHKyaKFo"
      },
      "outputs": [],
      "source": [
        "# Specify directory to save embeddings\n",
        "import os\n",
        "drive_save_dir = f\"/content/drive/My Drive/UvA/M Thesis/Data/{dataset_dir}/Embeddings/{embedding_method}/{image_size}\"\n",
        "\n",
        "os.makedirs(drive_save_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LiyfRxO5IC04",
        "outputId": "ed58fa41-8143-45b3-ceff-6dd6fca13511"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://github.com/facebookresearch/dinov2/zipball/main\" to /root/.cache/torch/hub/main.zip\n",
            "/root/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/swiglu_ffn.py:51: UserWarning: xFormers is not available (SwiGLU)\n",
            "  warnings.warn(\"xFormers is not available (SwiGLU)\")\n",
            "/root/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:33: UserWarning: xFormers is not available (Attention)\n",
            "  warnings.warn(\"xFormers is not available (Attention)\")\n",
            "/root/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/block.py:40: UserWarning: xFormers is not available (Block)\n",
            "  warnings.warn(\"xFormers is not available (Block)\")\n",
            "Downloading: \"https://dl.fbaipublicfiles.com/dinov2/dinov2_vits14/dinov2_vits14_pretrain.pth\" to /root/.cache/torch/hub/checkpoints/dinov2_vits14_pretrain.pth\n",
            "100%|██████████| 84.2M/84.2M [00:00<00:00, 148MB/s]\n"
          ]
        }
      ],
      "source": [
        "# DINOv2 version to use.\n",
        "import torch\n",
        "\n",
        "lvm = torch.hub.load('facebookresearch/dinov2', DINO_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "eCBbxZdzDEZe",
        "outputId": "c6e3a190-1257-42f4-e9b6-55ebf9d3295c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/AK fish'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "# Copy dataset\n",
        "import shutil\n",
        "\n",
        "data_source = f\"/content/drive/My Drive/UvA/M Thesis/Data/{dataset_dir}\"\n",
        "data_dir = f\"/content/{dataset_dir}\"\n",
        "\n",
        "# Copy the folder to destination\n",
        "shutil.copytree(data_source, data_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "lsHsXz3_D5e9"
      },
      "outputs": [],
      "source": [
        "# Auxilliary functions.\n",
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "def calculate_new_dimensions(frame, image_size, patch_size):\n",
        "  '''\n",
        "  Calculates the downsampled dimensions of images.\n",
        "\n",
        "  :param: frame: the frame to downsample.\n",
        "  :param: image_size: the downsampled length of the smaller side of the frame.\n",
        "  :param: patch_size: the patch size of the LVM which will be used for embeddings.\n",
        "  '''\n",
        "\n",
        "  height, width, _ = frame.shape\n",
        "\n",
        "  # Calculate new image dimensions.\n",
        "  if width > height:\n",
        "    new_width = image_size\n",
        "    new_height = int(height * image_size / width)\n",
        "\n",
        "  else:\n",
        "    new_height = image_size\n",
        "    new_width = int(width * image_size / height)\n",
        "\n",
        "  # Ensure that both dimensions are multiples of the patch size.\n",
        "  if new_width % patch_size != 0:\n",
        "    new_width = (new_width // patch_size) * patch_size\n",
        "\n",
        "  if new_height % patch_size != 0:\n",
        "    new_height = (new_height // patch_size) * patch_size\n",
        "\n",
        "  return new_width, new_height\n",
        "\n",
        "def generate_default_embedding(frame, transform):\n",
        "  '''\n",
        "  Generates a default embedding of a frame.\n",
        "\n",
        "  :param: frame: the frame as a numpy array.\n",
        "  :param: transform: the torchvision transforms object with the necessary transformations.\n",
        "  :return: the embedding as a numpy array.\n",
        "  '''\n",
        "\n",
        "  img = transform(Image.fromarray(frame))[:3].unsqueeze(0)\n",
        "\n",
        "  with torch.no_grad():\n",
        "    # Note: lvm is the loaded large vision model to generate the embedding.\n",
        "    embedding = lvm(img)[0]\n",
        "\n",
        "  return embedding.squeeze().numpy()\n",
        "\n",
        "# Method dictionary for later easy of use.\n",
        "embedding_methods = {'clf' : generate_default_embedding}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "kEXCIyn2FK6y"
      },
      "outputs": [],
      "source": [
        "# Read frame selection table.\n",
        "import pandas as pd\n",
        "\n",
        "selection_table_name = f'{dataset_name}_{frame_selection_method}.csv'\n",
        "\n",
        "frames_df = pd.read_csv(f'/content/{dataset_dir}/Selected frames/{selection_table_name}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wUinXqCYxE_M",
        "outputId": "ebbf26dd-93f4-4728-a47b-449bece8e69a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of videos: 887\n",
            "Progress: 100.00%\n"
          ]
        }
      ],
      "source": [
        "import torchvision.transforms as T\n",
        "import cv2\n",
        "import numpy as np\n",
        "from IPython.display import clear_output\n",
        "\n",
        "# Select method of feature extraction.\n",
        "generate_embedding = embedding_methods[feature_extraction]\n",
        "\n",
        "save_dir = f'{data_dir}/Embeddings/{embedding_method}/{image_size}'\n",
        "os.makedirs(save_dir, exist_ok = True)\n",
        "\n",
        "nr_videos = len(frames_df.index)\n",
        "\n",
        "failed_frames = []\n",
        "\n",
        "transformation_set = False    # Marks if image transformation process has been set.\n",
        "\n",
        "for i, row in frames_df.iterrows():\n",
        "    # Show progress\n",
        "    clear_output(wait=True)\n",
        "    print(f'Number of videos: {nr_videos}')\n",
        "    print(f'Progress: {(i + 1 )/ nr_videos * 100:.2f}%')\n",
        "\n",
        "    video_file = row['video']\n",
        "    frame_indices = eval(row['frames'])\n",
        "\n",
        "    video_name = video_file.replace(\".mp4\", \"\")\n",
        "\n",
        "    video_path = f'/content/{dataset_dir}/Clips/{video_file}'\n",
        "\n",
        "    video_save_dir = f'{save_dir}/{video_name}'\n",
        "\n",
        "    if not os.path.exists(video_save_dir):\n",
        "      os.makedirs(video_save_dir)\n",
        "\n",
        "    drive_video_dir = f'{drive_save_dir}/{video_name}'\n",
        "\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "    for frame_idx in frame_indices:\n",
        "      # Destination to save embedding\n",
        "      save_path = f'{video_save_dir}/{video_name}_{frame_idx}.npy'\n",
        "\n",
        "\n",
        "      # Check if embedding already exists and skip if it does (optional).\n",
        "      if skip_existing:\n",
        "\n",
        "        # Skip if embedding already exists in Drive.\n",
        "        if os.path.exists(f'{drive_save_dir}/{video_name}_{frame_idx}.npy'):\n",
        "          continue\n",
        "\n",
        "        # Skip if already exists in runtime.\n",
        "        elif os.path.exists(save_path):\n",
        "\n",
        "          # Copy to Drive if immediate copy is enabled.\n",
        "          if immediate_copy:\n",
        "            if not os.path.exists(drive_video_dir):\n",
        "              os.makedirs(drive_video_dir)\n",
        "\n",
        "            shutil.copy(save_path, f'{drive_video_dir}/{video_name}_{frame_idx}.npy')\n",
        "\n",
        "          continue\n",
        "\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n",
        "        ret, frame = cap.read()\n",
        "\n",
        "        if ret:\n",
        "          if not transformation_set:\n",
        "            new_width, new_height = calculate_new_dimensions(frame, image_size, patch_size)\n",
        "\n",
        "            # Set transformations to use\n",
        "            transform = T.Compose([\n",
        "                T.Resize((new_height, new_width)),\n",
        "                T.ToTensor()\n",
        "                ])\n",
        "\n",
        "            transformation_set = True\n",
        "\n",
        "          # Convert frame to RGB (from BGR)\n",
        "          frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "          embedding = generate_embedding(frame, transform)\n",
        "\n",
        "          # Save embedding as a NumPy array\n",
        "          np.save(save_path, embedding)\n",
        "\n",
        "          # Copy file to Drive (optional).\n",
        "          if immediate_copy:\n",
        "            if not os.path.exists(drive_video_dir):\n",
        "              os.makedirs(drive_video_dir)\n",
        "\n",
        "            shutil.copy(save_path, f'{drive_video_dir}/{video_name}_{frame_idx}.npy')\n",
        "\n",
        "          # Clearing memory\n",
        "          frame = None\n",
        "          embedding = None\n",
        "\n",
        "        else:\n",
        "          failed_frames.append((video_file, frame_idx))\n",
        "          print('WARNING: failed to read frame.')\n",
        "\n",
        "    # Clearing memory.\n",
        "    cap.release()\n",
        "    cap = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JDsD0PcySK3n",
        "outputId": "9d5969e8-5b16-4083-d1d6-275c27b5a084"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Success! All frames read.\n"
          ]
        }
      ],
      "source": [
        "if len(failed_frames) > 0:\n",
        "  print('WARNING: some frames were not read.')\n",
        "\n",
        "else:\n",
        "  print('Success! All frames read.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zLS0xeM6ytgj",
        "outputId": "9207c080-82b5-4822-8f87-ae0f0ec4e5f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embeddings have already been copied to Drive.\n"
          ]
        }
      ],
      "source": [
        "# Copy embeddings to Drive if they have not been already.\n",
        "if not immediate_copy:\n",
        "  shutil.copytree(save_dir, drive_save_dir)\n",
        "\n",
        "else:\n",
        "  print('Embeddings have already been copied to Drive.')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if there are any missing frame embeddings\n",
        "nr_frames = int(frame_selection_method.split('_')[-1])    # Correct number of embedded frames per video\n",
        "\n",
        "frames_df = pd.read_csv(f'/content/{dataset_dir}/Selected frames/{selection_table_name}', \\\n",
        "                                    index_col = 'video')\n",
        "\n",
        "for item in os.listdir(drive_save_dir):\n",
        "  item_path = f'{drive_save_dir}/{item}'\n",
        "\n",
        "  if os.path.isdir(item_path):\n",
        "    if len(os.listdir(item_path)) != nr_frames:\n",
        "      print('VIDEO:', item)\n",
        "\n",
        "      selected_frames = eval(frames_df['frames'][f'{item}.mp4'])\n",
        "\n",
        "      found_frames = []\n",
        "\n",
        "      for embedding in os.listdir(item_path):\n",
        "        found_frames.append(int(embedding.split('_')[-1].replace('.npy', '')))\n",
        "\n",
        "      for i in selected_frames:\n",
        "        if i not in found_frames:\n",
        "          print('I')\n",
        "\n",
        "      print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4AeVKA_AA9JK",
        "outputId": "fdaf9be8-fb6a-43f0-883f-8a34c4a8f655"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VIDEO: UVHZNUPH\n",
            "\n",
            "VIDEO: SZPVDMHZ\n",
            "\n",
            "VIDEO: QDSYAFGA\n",
            "\n",
            "VIDEO: ISOMHLHH\n",
            "\n",
            "VIDEO: XZTDNQCJ\n",
            "\n",
            "VIDEO: BJECMPAB\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN3JDXB8kllZlR4V6E1Dmu3",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}