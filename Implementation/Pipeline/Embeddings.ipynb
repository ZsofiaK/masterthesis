{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMNhtj/MnDVURZ1cMj5Yard",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ZsofiaK/masterthesis/blob/main/Implementation/Pipeline/Embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DINOv2 embeddings\n",
        "\n",
        "This notebook generates and saves DINOv2 embeddings of pre-selected frames from a video dataset."
      ],
      "metadata": {
        "id": "ETboycQ3Cts4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mYW7FqsXCsqA",
        "outputId": "1c2dc21b-6e50-4de6-f279-8c303fc1e3f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Mount drive.\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_dir = 'Fish clips'\n",
        "\n",
        "dataset_name = 'fishClips'\n",
        "\n",
        "frame_selection_method = 'motionAbsdiff_10'\n",
        "\n",
        "embedding_method = 'default'"
      ],
      "metadata": {
        "id": "_zHzpB0hGku7"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DINOv2 version to use.\n",
        "import torch\n",
        "\n",
        "lvm = torch.hub.load('facebookresearch/dinov2', 'dinov2_vits14')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LiyfRxO5IC04",
        "outputId": "e240defc-7cd2-4648-ba0c-a362398488a6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://github.com/facebookresearch/dinov2/zipball/main\" to /root/.cache/torch/hub/main.zip\n",
            "/root/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/swiglu_ffn.py:51: UserWarning: xFormers is not available (SwiGLU)\n",
            "  warnings.warn(\"xFormers is not available (SwiGLU)\")\n",
            "/root/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:33: UserWarning: xFormers is not available (Attention)\n",
            "  warnings.warn(\"xFormers is not available (Attention)\")\n",
            "/root/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/block.py:40: UserWarning: xFormers is not available (Block)\n",
            "  warnings.warn(\"xFormers is not available (Block)\")\n",
            "Downloading: \"https://dl.fbaipublicfiles.com/dinov2/dinov2_vits14/dinov2_vits14_pretrain.pth\" to /root/.cache/torch/hub/checkpoints/dinov2_vits14_pretrain.pth\n",
            "100%|██████████| 84.2M/84.2M [00:00<00:00, 113MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Transformations to use\n",
        "import torchvision.transforms as T\n",
        "\n",
        "transform = T.Compose([\n",
        "    T.Resize(224),\n",
        "    T.CenterCrop(224),\n",
        "    T.ToTensor()\n",
        "    ])"
      ],
      "metadata": {
        "id": "WFlW0ughJNIx"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Copy dataset\n",
        "import shutil\n",
        "\n",
        "data_source = f\"/content/drive/My Drive/UvA/M Thesis/Data/{dataset_dir}\"\n",
        "data_dir = \"/content/Fish clips\"\n",
        "\n",
        "# Copy the folder to destination\n",
        "shutil.copytree(data_source, data_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "eCBbxZdzDEZe",
        "outputId": "739c9ada-2a15-4899-ad4e-66b8f3432d7b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/Fish clips'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Auxilliary functions.\n",
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "def generate_default_embedding(frame, transform):\n",
        "  '''\n",
        "  Generates a default embedding of a frame.\n",
        "\n",
        "  :param: frame: the frame as a numpy array.\n",
        "  :param: transform: the torchvision transforms object with the necessary transformations.\n",
        "  :return: the embedding as a numpy array.\n",
        "  '''\n",
        "\n",
        "  img = transform(Image.fromarray(frame))[:3].unsqueeze(0)\n",
        "\n",
        "  with torch.no_grad():\n",
        "    # Note: lvm is the loaded large vision model to generate the embedding.\n",
        "    embedding = lvm(img)[0]\n",
        "\n",
        "  return embedding.squeeze().numpy()\n",
        "\n",
        "# Method dictionary for later easy of use.\n",
        "embedding_methods = {'default' : generate_default_embedding}"
      ],
      "metadata": {
        "id": "lsHsXz3_D5e9"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read frame selection table.\n",
        "import pandas as pd\n",
        "\n",
        "selection_table_name = f'{dataset_name}_{frame_selection_method}.csv'\n",
        "\n",
        "frames_df = pd.read_csv(f'/content/{dataset_dir}/Selected frames/{selection_table_name}')"
      ],
      "metadata": {
        "id": "kEXCIyn2FK6y"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from IPython.display import clear_output\n",
        "\n",
        "generate_embedding = embedding_methods[embedding_method]\n",
        "\n",
        "save_dir = f'{data_dir}/Embeddings/{frame_selection_method}/{embedding_method}'\n",
        "os.makedirs(save_dir, exist_ok = True)\n",
        "\n",
        "nr_videos = len(frames_df.index)\n",
        "\n",
        "failed_frames = []\n",
        "\n",
        "for i, row in frames_df.iterrows():\n",
        "    video_file = row['video']\n",
        "    frame_indices = eval(row['frames'])\n",
        "\n",
        "    video_path = f'/content/{dataset_dir}/Clips/{video_file}'\n",
        "\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "    embeddings = []\n",
        "\n",
        "    for frame_idx in frame_indices:\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n",
        "        ret, frame = cap.read()\n",
        "\n",
        "        if ret:\n",
        "          # Convert frame to RGB (from BGR)\n",
        "          frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "          embedding = generate_embedding(frame, transform)\n",
        "\n",
        "          embeddings.append(embedding)\n",
        "\n",
        "          # Clearing memory\n",
        "          frame = None\n",
        "          embedding = None\n",
        "\n",
        "          clear_output(wait=True)\n",
        "          print(f'Number of videos: {nr_videos}')\n",
        "          print(f'Progress: {(i + 1 )/ nr_videos * 100:.2f}%')\n",
        "\n",
        "        else:\n",
        "          failed_frames.append((video_file, frame_idx))\n",
        "\n",
        "          clear_output(wait=True)\n",
        "          print(f'Number of videos: {nr_videos}')\n",
        "          print(f'Progress: {(i + 1 )/ nr_videos * 100:.2f}%')\n",
        "          print('WARNING: failed to read frame.')\n",
        "\n",
        "    # Clearing memory.\n",
        "    cap.release()\n",
        "    cap = None\n",
        "\n",
        "    embeddings = np.concatenate(embeddings, axis=0)\n",
        "\n",
        "    # Save concatenated embeddings as a NumPy array\n",
        "    video_name = video_file.replace(\".mp4\", \"\")\n",
        "\n",
        "    save_path = f'{save_dir}/{video_name}.npy'\n",
        "\n",
        "    np.save(save_path, embeddings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wUinXqCYxE_M",
        "outputId": "80f94dd5-4f90-492b-ae13-6382d19c2ebb"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of videos: 220\n",
            "Progress: 100.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if len(failed_frames) > 0:\n",
        "  print('WARNING: some frames were not read.')\n",
        "\n",
        "else:\n",
        "  print('Success! All frames read.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JDsD0PcySK3n",
        "outputId": "d8c91166-95d8-471c-d3d2-720673459904"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Success! All frames read.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Copy embeddings to Drive.\n",
        "drive_dir = f\"/content/drive/My Drive/UvA/M Thesis/Data/{dataset_dir}/Embeddings/{frame_selection_method}/{embedding_method}\"\n",
        "\n",
        "shutil.copytree(save_dir, drive_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "zLS0xeM6ytgj",
        "outputId": "38cecff8-16d8-4225-8f5a-cfbf47ffe809"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/My Drive/UvA/M Thesis/Data/Fish clips/Embeddings/motionAbsdiff_10/default'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    }
  ]
}