{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN9KUBZ8aYkjSfF6sY2zLpr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ZsofiaK/masterthesis/blob/main/Implementation/Pipeline/Embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DINOv2 embeddings\n",
        "\n",
        "This notebook generates and saves DINOv2 embeddings of pre-selected frames from a video dataset."
      ],
      "metadata": {
        "id": "ETboycQ3Cts4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "mYW7FqsXCsqA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2549e8b-79fc-4126-b53c-fce356169159"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Mount Drive.\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_dir = 'AK fish'\n",
        "\n",
        "dataset_name = 'AK-fish'\n",
        "\n",
        "frame_selection_method = 'motionAbsdiff_10'\n",
        "\n",
        "DINO_model = 'dinov2_vits14'\n",
        "\n",
        "feature_extraction = 'clf'\n",
        "\n",
        "embedding_method = f'{DINO_model}-{feature_extraction}'.replace('_', '-')"
      ],
      "metadata": {
        "id": "_zHzpB0hGku7"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DINOv2 version to use.\n",
        "import torch\n",
        "\n",
        "lvm = torch.hub.load('facebookresearch/dinov2', DINO_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LiyfRxO5IC04",
        "outputId": "a71f5ab4-aa80-4b96-c41c-85bf8a839297"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/facebookresearch_dinov2_main\n",
            "Downloading: \"https://dl.fbaipublicfiles.com/dinov2/dinov2_vits14/dinov2_vits14_pretrain.pth\" to /root/.cache/torch/hub/checkpoints/dinov2_vits14_pretrain.pth\n",
            "100%|██████████| 84.2M/84.2M [00:03<00:00, 27.0MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Transformations to use\n",
        "import torchvision.transforms as T\n",
        "\n",
        "transform = T.Compose([\n",
        "    T.Resize(224),\n",
        "    T.CenterCrop(224),\n",
        "    T.ToTensor()\n",
        "    ])"
      ],
      "metadata": {
        "id": "WFlW0ughJNIx"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Copy dataset\n",
        "import shutil\n",
        "\n",
        "data_source = f\"/content/drive/My Drive/UvA/M Thesis/Data/{dataset_dir}\"\n",
        "data_dir = f\"/content/{dataset_dir}\"\n",
        "\n",
        "# Copy the folder to destination\n",
        "shutil.copytree(data_source, data_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "eCBbxZdzDEZe",
        "outputId": "fad6a774-8604-449e-f1cd-69b6c4fa3f86"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/AK fish'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Auxilliary functions.\n",
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "def generate_default_embedding(frame, transform):\n",
        "  '''\n",
        "  Generates a default embedding of a frame.\n",
        "\n",
        "  :param: frame: the frame as a numpy array.\n",
        "  :param: transform: the torchvision transforms object with the necessary transformations.\n",
        "  :return: the embedding as a numpy array.\n",
        "  '''\n",
        "\n",
        "  img = transform(Image.fromarray(frame))[:3].unsqueeze(0)\n",
        "\n",
        "  with torch.no_grad():\n",
        "    # Note: lvm is the loaded large vision model to generate the embedding.\n",
        "    embedding = lvm(img)[0]\n",
        "\n",
        "  return embedding.squeeze().numpy()\n",
        "\n",
        "# Method dictionary for later easy of use.\n",
        "embedding_methods = {'clf' : generate_default_embedding}"
      ],
      "metadata": {
        "id": "lsHsXz3_D5e9"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read frame selection table.\n",
        "import pandas as pd\n",
        "\n",
        "selection_table_name = f'{dataset_name}_{frame_selection_method}.csv'\n",
        "\n",
        "frames_df = pd.read_csv(f'/content/{dataset_dir}/Selected frames/{selection_table_name}')"
      ],
      "metadata": {
        "id": "kEXCIyn2FK6y"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from IPython.display import clear_output\n",
        "\n",
        "# Select method of feature extraction.\n",
        "generate_embedding = embedding_methods[feature_extraction]\n",
        "\n",
        "save_dir = f'{data_dir}/Embeddings/{frame_selection_method}/{embedding_method}'\n",
        "os.makedirs(save_dir, exist_ok = True)\n",
        "\n",
        "nr_videos = len(frames_df.index)\n",
        "\n",
        "failed_frames = []\n",
        "\n",
        "for i, row in frames_df.iterrows():\n",
        "    video_file = row['video']\n",
        "    frame_indices = eval(row['frames'])\n",
        "\n",
        "    video_path = f'/content/{dataset_dir}/Clips/{video_file}'\n",
        "\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "    embeddings = []\n",
        "\n",
        "    for frame_idx in frame_indices:\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n",
        "        ret, frame = cap.read()\n",
        "\n",
        "        if ret:\n",
        "          # Convert frame to RGB (from BGR)\n",
        "          frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "          embedding = generate_embedding(frame, transform)\n",
        "\n",
        "          embeddings.append(embedding)\n",
        "\n",
        "          # Clearing memory\n",
        "          frame = None\n",
        "          embedding = None\n",
        "\n",
        "          clear_output(wait=True)\n",
        "          print(f'Number of videos: {nr_videos}')\n",
        "          print(f'Progress: {(i + 1 )/ nr_videos * 100:.2f}%')\n",
        "\n",
        "        else:\n",
        "          failed_frames.append((video_file, frame_idx))\n",
        "\n",
        "          clear_output(wait=True)\n",
        "          print(f'Number of videos: {nr_videos}')\n",
        "          print(f'Progress: {(i + 1 )/ nr_videos * 100:.2f}%')\n",
        "          print('WARNING: failed to read frame.')\n",
        "\n",
        "    # Clearing memory.\n",
        "    cap.release()\n",
        "    cap = None\n",
        "\n",
        "    embeddings = np.concatenate(embeddings, axis=0)\n",
        "\n",
        "    # Save concatenated embeddings as a NumPy array\n",
        "    video_name = video_file.replace(\".mp4\", \"\")\n",
        "\n",
        "    save_path = f'{save_dir}/{video_name}.npy'\n",
        "\n",
        "    np.save(save_path, embeddings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wUinXqCYxE_M",
        "outputId": "805df75c-302f-4a2a-fb97-9f8cb1aa34ca"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of videos: 887\n",
            "Progress: 100.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if len(failed_frames) > 0:\n",
        "  print('WARNING: some frames were not read.')\n",
        "\n",
        "else:\n",
        "  print('Success! All frames read.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JDsD0PcySK3n",
        "outputId": "08f8aaf3-7b88-4ff0-95fc-2faac4acba15"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Success! All frames read.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Copy embeddings to Drive.\n",
        "drive_dir = f\"/content/drive/My Drive/UvA/M Thesis/Data/{dataset_dir}/Embeddings/{frame_selection_method}/{embedding_method}\"\n",
        "\n",
        "shutil.copytree(save_dir, drive_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "zLS0xeM6ytgj",
        "outputId": "48aeca87-41e0-4d5b-cb65-13357d0d1f89"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/My Drive/UvA/M Thesis/Data/AK fish/Embeddings/motionAbsdiff_10/dinov2-vits14-clf'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    }
  ]
}