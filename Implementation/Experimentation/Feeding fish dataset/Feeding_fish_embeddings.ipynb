{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyNhh6ivxnOeA1uUt2vKS4XZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ZsofiaK/masterthesis/blob/main/Implementation/Experimentation/Feeding%20fish%20dataset/Feeding_fish_embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generating DINOv2 embeddings for the feeding fish dataset"
      ],
      "metadata": {
        "id": "26WeA1mwjPjW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bNfTDClmjNEy",
        "outputId": "ab73a411-c389-49b9-8227-86ef84f060d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Mount Drive.\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_dir = 'Feeding fish'\n",
        "\n",
        "DINO_model = 'dinov2_vitb14_reg'\n",
        "\n",
        "feature_extraction = 'clf'\n",
        "\n",
        "image_size = 196    # Size to use when downsampling the frames (shorter side).\n",
        "\n",
        "patch_size = 14     # A characteristic of the DINOv2 model.\n",
        "\n",
        "embedding_method = f'{DINO_model}-{feature_extraction}'.replace('_', '-')\n",
        "\n",
        "immediate_copy = True   # If the embeddings should immediately be copied to Drive.\n",
        "\n",
        "skip_existing = True    # If existing embeddings should not be calculated again."
      ],
      "metadata": {
        "id": "VD9GGrGwjhLQ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify directory to save embeddings\n",
        "import os\n",
        "drive_save_dir = f\"/content/drive/My Drive/UvA/M Thesis/Data/{dataset_dir}/Embeddings/{embedding_method}/{image_size}\"\n",
        "\n",
        "os.makedirs(drive_save_dir, exist_ok=True)"
      ],
      "metadata": {
        "id": "5yqYwajnjyWh"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DINOv2 version to use.\n",
        "import torch\n",
        "\n",
        "lvm = torch.hub.load('facebookresearch/dinov2', DINO_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H68yVNvLj2-d",
        "outputId": "5b211a74-656d-4db2-b5d4-58933e0b785c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://github.com/facebookresearch/dinov2/zipball/main\" to /root/.cache/torch/hub/main.zip\n",
            "/root/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/swiglu_ffn.py:51: UserWarning: xFormers is not available (SwiGLU)\n",
            "  warnings.warn(\"xFormers is not available (SwiGLU)\")\n",
            "/root/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:33: UserWarning: xFormers is not available (Attention)\n",
            "  warnings.warn(\"xFormers is not available (Attention)\")\n",
            "/root/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/block.py:40: UserWarning: xFormers is not available (Block)\n",
            "  warnings.warn(\"xFormers is not available (Block)\")\n",
            "Downloading: \"https://dl.fbaipublicfiles.com/dinov2/dinov2_vitb14/dinov2_vitb14_reg4_pretrain.pth\" to /root/.cache/torch/hub/checkpoints/dinov2_vitb14_reg4_pretrain.pth\n",
            "100%|██████████| 330M/330M [00:02<00:00, 166MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify dataset directory\n",
        "data_dir = f\"/content/drive/MyDrive/UvA/M Thesis/Data/{dataset_dir}\""
      ],
      "metadata": {
        "id": "YP5R-h3Gj6i5"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Auxilliary functions.\n",
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "def calculate_new_dimensions(frame, image_size, patch_size):\n",
        "  '''\n",
        "  Calculates the downsampled dimensions of images.\n",
        "\n",
        "  :param: frame: the frame to downsample.\n",
        "  :param: image_size: the downsampled length of the smaller side of the frame.\n",
        "  :param: patch_size: the patch size of the LVM which will be used for embeddings.\n",
        "  '''\n",
        "\n",
        "  height, width, _ = frame.shape\n",
        "\n",
        "  # Calculate new image dimensions.\n",
        "  if width > height:\n",
        "    new_width = image_size\n",
        "    new_height = int(height * image_size / width)\n",
        "\n",
        "  else:\n",
        "    new_height = image_size\n",
        "    new_width = int(width * image_size / height)\n",
        "\n",
        "  # Ensure that both dimensions are multiples of the patch size.\n",
        "  if new_width % patch_size != 0:\n",
        "    new_width = (new_width // patch_size) * patch_size\n",
        "\n",
        "  if new_height % patch_size != 0:\n",
        "    new_height = (new_height // patch_size) * patch_size\n",
        "\n",
        "  return new_width, new_height\n",
        "\n",
        "def generate_default_embedding(frame, transform):\n",
        "  '''\n",
        "  Generates a default embedding of a frame.\n",
        "\n",
        "  :param: frame: the frame as a numpy array.\n",
        "  :param: transform: the torchvision transforms object with the necessary transformations.\n",
        "  :return: the embedding as a numpy array.\n",
        "  '''\n",
        "\n",
        "  img = transform(Image.fromarray(frame))[:3].unsqueeze(0)\n",
        "\n",
        "  with torch.no_grad():\n",
        "    # Note: lvm is the loaded large vision model to generate the embedding.\n",
        "    embedding = lvm(img)[0]\n",
        "\n",
        "  return embedding.squeeze().numpy()\n",
        "\n",
        "# Method dictionary for later easy of use.\n",
        "embedding_methods = {'clf' : generate_default_embedding}"
      ],
      "metadata": {
        "id": "7di-3Y84kAWC"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read table with selected frames.\n",
        "import pandas as pd\n",
        "\n",
        "frames_path = f'{data_dir}/clips.csv'\n",
        "\n",
        "frames_df = pd.read_csv(frames_path)"
      ],
      "metadata": {
        "id": "nMCJjpOQkQND"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.transforms as T\n",
        "import cv2\n",
        "import numpy as np\n",
        "from IPython.display import clear_output\n",
        "import shutil\n",
        "\n",
        "# Select method of feature extraction.\n",
        "generate_embedding = embedding_methods[feature_extraction]\n",
        "\n",
        "save_dir = f'/content/Embeddings/{embedding_method}/{image_size}'\n",
        "os.makedirs(save_dir, exist_ok = True)\n",
        "\n",
        "nr_frames = len(frames_df.index)\n",
        "\n",
        "failed_frames = []\n",
        "\n",
        "transformation_set = False    # Marks if image transformation process has been set.\n",
        "\n",
        "for i, row in frames_df.iterrows():\n",
        "  # Show progress\n",
        "  clear_output(wait=True)\n",
        "  print(f'Number of frames: {nr_frames}')\n",
        "  print(f'Progress: {(i+1)/nr_frames * 100:.2f}%')\n",
        "\n",
        "  video_file = row['video'].replace('.txt', '.mp4')\n",
        "  frame_idx = int(row['frame'])\n",
        "\n",
        "  video_name = video_file.replace(\".mp4\", \"\")\n",
        "\n",
        "  video_path = f'{data_dir}/Original data/Videos/{video_file}'\n",
        "\n",
        "  video_save_dir = f'{save_dir}/{video_name}'\n",
        "\n",
        "  if not os.path.exists(video_save_dir):\n",
        "    os.makedirs(video_save_dir)\n",
        "\n",
        "  drive_video_dir = f'{drive_save_dir}/{video_name}'\n",
        "\n",
        "  cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "  # Destination to save embedding\n",
        "  save_path = f'{video_save_dir}/{video_name}_{frame_idx}.npy'\n",
        "\n",
        "  # Check if embedding already exists and skip if it does (optional).\n",
        "  if skip_existing:\n",
        "\n",
        "    # Skip if embedding already exists in Drive.\n",
        "    if os.path.exists(f'{drive_video_dir}/{video_name}_{frame_idx}.npy'):\n",
        "      print('Exists in Drive')\n",
        "      continue\n",
        "\n",
        "    # Skip if already exists in runtime.\n",
        "    elif os.path.exists(save_path):\n",
        "      print('Exists in runtime')\n",
        "\n",
        "      # Copy to Drive if immediate copy is enabled.\n",
        "      if immediate_copy:\n",
        "        if not os.path.exists(drive_video_dir):\n",
        "          os.makedirs(drive_video_dir)\n",
        "\n",
        "        shutil.copy(save_path, f'{drive_video_dir}/{video_name}_{frame_idx}.npy')\n",
        "\n",
        "      continue\n",
        "\n",
        "    cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n",
        "    ret, frame = cap.read()\n",
        "\n",
        "    if ret:\n",
        "      if not transformation_set:\n",
        "        new_width, new_height = calculate_new_dimensions(frame, image_size, patch_size)\n",
        "\n",
        "        # Set transformations to use\n",
        "        transform = T.Compose([\n",
        "            T.Resize((new_height, new_width)),\n",
        "            T.ToTensor()\n",
        "            ])\n",
        "\n",
        "        transformation_set = True\n",
        "\n",
        "      # Convert frame to RGB (from BGR)\n",
        "      frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "      embedding = generate_embedding(frame, transform)\n",
        "\n",
        "      # Save embedding as a NumPy array\n",
        "      np.save(save_path, embedding)\n",
        "\n",
        "      # Copy file to Drive (optional).\n",
        "      if immediate_copy:\n",
        "        if not os.path.exists(drive_video_dir):\n",
        "          os.makedirs(drive_video_dir)\n",
        "\n",
        "        shutil.copy(save_path, f'{drive_video_dir}/{video_name}_{frame_idx}.npy')\n",
        "\n",
        "      # Clearing memory\n",
        "      frame = None\n",
        "      embedding = None\n",
        "\n",
        "    else:\n",
        "      failed_frames.append((video_file, frame_idx))\n",
        "\n",
        "  # Clearing memory.\n",
        "  cap.release()\n",
        "  cap = None"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JhpRGfobkBEH",
        "outputId": "f1a03059-f663-450c-c07f-0c27e4cd7eee"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of frames: 3348\n",
            "Progress: 100.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if len(failed_frames) > 0:\n",
        "  print(f'WARNING: {len(failed_frames)} frames were not read.\\n')\n",
        "\n",
        "  for i, failed in enumerate(failed_frames):\n",
        "    if i < 5:\n",
        "      print('VIDEO', failed[0])\n",
        "      print('INDEX', failed[1])\n",
        "      print()\n",
        "\n",
        "else:\n",
        "  print('Success! All frames read.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Ir8nonxmuTn",
        "outputId": "ecf507d2-aabd-4e1c-f5da-16d5315b69f3"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: 4 frames were not read.\n",
            "\n",
            "VIDEO tracked_fish_clip_2_Jan_11_part_2_track_1625.mp4\n",
            "INDEX 38\n",
            "\n",
            "VIDEO tracked_fish_clip_2_Jan_11_track_946.mp4\n",
            "INDEX 8\n",
            "\n",
            "VIDEO tracked_fish_3_track_25.mp4\n",
            "INDEX 2\n",
            "\n",
            "VIDEO tracked_fish_clip_2_Jan_11_part_2_track_1625.mp4\n",
            "INDEX 195\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Copy embeddings to Drive if they have not been already.\n",
        "if not immediate_copy:\n",
        "  shutil.copytree(save_dir, drive_save_dir)\n",
        "\n",
        "else:\n",
        "  print('Embeddings have already been copied to Drive.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D5riLksKmxO5",
        "outputId": "911057a8-c908-4b12-dbd3-cc46e7a5c141"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embeddings have already been copied to Drive.\n"
          ]
        }
      ]
    }
  ]
}