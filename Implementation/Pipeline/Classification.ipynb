{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyPVKyXjLsQF5Mls9oSQQHbG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ZsofiaK/masterthesis/blob/main/Implementation/Pipeline/Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classification (binary action recognition)"
      ],
      "metadata": {
        "id": "-JvN_Sh01GXI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting up requirements"
      ],
      "metadata": {
        "id": "zaYVIbZ2F5WY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting up dictionaries for later ease of use.\n",
        "dataset_dict = {'fishClips' : 'Fish clips', 'AK-fish' : 'AK fish'}\n",
        "\n",
        "input_sizes_dict = {'dinov2-vits14-clf' : 384, 'dinov2-vitg14-clf' : 1536,\n",
        "                    'dinov2-vits14-reg-clf' : 384, 'dinov2-vitg14-reg-clf' : 1536}"
      ],
      "metadata": {
        "id": "-6SM6II0FnTh"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the dataset, model and embedding specifics for the classification.\n",
        "\n",
        "dataset_name = 'AK-fish'\n",
        "\n",
        "dataset_dir = dataset_dict[dataset_name]\n",
        "\n",
        "image_size = 448    # The size of the embedded images.\n",
        "\n",
        "frame_selection_method = 'motionAbsdiff_10'\n",
        "\n",
        "embedding_method = 'dinov2-vitg14-reg-clf'\n",
        "\n",
        "clf_name = 'ShallowNetwork'\n",
        "\n",
        "val_score = 'roc_auc'  # Score to use during cross-validation.\n",
        "\n",
        "seed = 23   # For reproducability in pseudo-randomness.\n",
        "\n",
        "nr_frames = int(frame_selection_method.split('_')[-1])\n",
        "\n",
        "input_size = input_sizes_dict[embedding_method] * nr_frames   # Size of the input vectors (embeddings)."
      ],
      "metadata": {
        "id": "MIJ71q9R1rOO"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting up parameters for cross validation.\n",
        "\n",
        "# Number of folds to use.\n",
        "cv_folds = 3\n",
        "\n",
        "# Parameter grids to use for the models.\n",
        "param_grid = { 'ShallowNetwork': {\n",
        "    'layers': [0, 1, 2, 3],\n",
        "    'dropout_rate': [0.0, 0.25, 0.5],\n",
        "    'learning_rate': [0.01, 0.001, 0.0001],\n",
        "    'pos_threshold' : [0.1, 0.15, 0.2, 0.5],  # Threshold for turning sigmoid prediction to binary label.\n",
        "    'epochs': [50],\n",
        "    'batch_size': [32]\n",
        "}\n",
        "}"
      ],
      "metadata": {
        "id": "FK_Z8GzHtuIp"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting up folder to save outputs.\n",
        "import os\n",
        "\n",
        "output_dir = 'Output'\n",
        "os.makedirs(output_dir, exist_ok=True)"
      ],
      "metadata": {
        "id": "t-XaFHjLwsB7"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hMDvjWVA1Fxv",
        "outputId": "65bca202-4862-40ae-9349-1df7463be16b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Mount Drive.\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify data source\n",
        "data_dir = f\"/content/drive/MyDrive/UvA/M Thesis/Data/{dataset_dir}\""
      ],
      "metadata": {
        "id": "LObn91mn17vm"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading data"
      ],
      "metadata": {
        "id": "h1IvtxOKF_iV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report\n",
        "from IPython.display import clear_output\n",
        "import shutil"
      ],
      "metadata": {
        "id": "FRRNvH6N2HMK"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify embedding locations\n",
        "embeddings_dir = f'{data_dir}/Embeddings/{embedding_method}/{image_size}'\n",
        "local_embeddings_dir = f'/content/{dataset_dir}/Embeddings/{embedding_method}/{image_size}'\n",
        "\n",
        "os.makedirs(local_embeddings_dir, exist_ok=True)"
      ],
      "metadata": {
        "id": "OlMCPLWjDxbD"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Copy embeddings to runtime.\n",
        "from IPython.display import clear_output\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "video_dirs = os.listdir(embeddings_dir)\n",
        "nr_videos = len(video_dirs)\n",
        "\n",
        "# Function to copy a single video directory\n",
        "def copy_video_dir(video_dir):\n",
        "    video_dir_path = os.path.join(embeddings_dir, video_dir)\n",
        "    local_video_dir_path = os.path.join(local_embeddings_dir, video_dir)\n",
        "\n",
        "    shutil.copytree(video_dir_path, local_video_dir_path)\n",
        "\n",
        "    return video_dir\n",
        "\n",
        "if len(os.listdir(local_embeddings_dir)) < nr_videos:    # Only copy if not already done.\n",
        "  # Progress tracker\n",
        "  progress = 0\n",
        "\n",
        "  # Copy directories in parallel\n",
        "  with ThreadPoolExecutor(max_workers=8) as executor:\n",
        "      futures = [executor.submit(copy_video_dir, video_dir) for video_dir in video_dirs]\n",
        "\n",
        "      for future in futures:\n",
        "        future.result()  # Wait for each future to complete\n",
        "\n",
        "        progress += 1\n",
        "\n",
        "        clear_output(wait=True)\n",
        "        print(f'Progress: {progress / nr_videos * 100:.2f}%')\n",
        "\n",
        "else:\n",
        "  print('Embeddings have already been copied.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g1Eqf9zIqttP",
        "outputId": "49bcd20e-93c9-4335-9bd7-6a202885b10e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embeddings have already been copied.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load video embeddings and labels\n",
        "clips_csv_path = f'{data_dir}/clips.csv'\n",
        "frame_selection_path = f'{data_dir}/Selected frames/{dataset_name}_{frame_selection_method}.csv'\n",
        "\n",
        "clips_df = pd.read_csv(clips_csv_path)\n",
        "frames_df = pd.read_csv(frame_selection_path, index_col='video')\n",
        "\n",
        "nr_clips = len(clips_df)\n",
        "progress = 0\n",
        "\n",
        "X_train = []  # Embeddings\n",
        "X_test = []\n",
        "y_train = []  # Labels\n",
        "y_test = []\n",
        "\n",
        "video_names_train = []  # Video names for saving predictions\n",
        "video_names_test = []\n",
        "\n",
        "not_found_embeddings = []\n",
        "\n",
        "for index, row in clips_df.iterrows():\n",
        "  skip_to_next = False\n",
        "\n",
        "  video_name = row['video'].replace('.mp4', '')\n",
        "  label = row['label']\n",
        "\n",
        "  embedding_path = f'{local_embeddings_dir}/{video_name}'\n",
        "\n",
        "  if video_name in video_names_train or video_name in video_names_test:\n",
        "    continue      # Skip embeddings which have already been read.\n",
        "\n",
        "  if not os.path.exists(embedding_path):\n",
        "    not_found_embeddings.append((video_name, 'all'))\n",
        "\n",
        "    progress += 1\n",
        "\n",
        "    clear_output(wait=True)\n",
        "    print(f'Number of videos: {nr_clips}')\n",
        "    print(f'Progress: {progress/nr_clips * 100:.2f}%')\n",
        "\n",
        "    continue\n",
        "\n",
        "  frames = eval(frames_df['frames'][f'{video_name}.mp4'])\n",
        "\n",
        "  embedding = []\n",
        "\n",
        "  for frame_idx in frames:\n",
        "    frame_embedding_path = f'{embedding_path}/{video_name}_{frame_idx}.npy'\n",
        "\n",
        "    if not os.path.exists(frame_embedding_path):\n",
        "      not_found_embeddings.append((video_name, frame_idx))\n",
        "\n",
        "      skip_to_next = True   # Skip to next video\n",
        "\n",
        "      break\n",
        "\n",
        "    try:\n",
        "      frame_embedding = np.load(frame_embedding_path)\n",
        "\n",
        "    except:\n",
        "      print(video_name, 'Unable to load an embedding.')\n",
        "      break\n",
        "\n",
        "    embedding.append(frame_embedding)\n",
        "\n",
        "  if skip_to_next:\n",
        "    progress += 1\n",
        "\n",
        "    clear_output(wait=True)\n",
        "    print(f'Number of videos: {nr_clips}')\n",
        "    print(f'Progress: {progress/nr_clips * 100:.2f}%')\n",
        "\n",
        "    continue\n",
        "\n",
        "  np_embedding = np.concatenate(embedding)\n",
        "\n",
        "  if row['type'] == 'train':\n",
        "    X_train.append(np_embedding)\n",
        "    y_train.append(label)\n",
        "    video_names_train.append(video_name)\n",
        "\n",
        "  elif row['type'] == 'test':\n",
        "    X_test.append(np_embedding)\n",
        "    y_test.append(label)\n",
        "    video_names_test.append(video_name)\n",
        "\n",
        "  progress += 1\n",
        "\n",
        "  clear_output(wait=True)\n",
        "  print(f'Number of videos: {nr_clips}')\n",
        "  print(f'Progress: {progress/nr_clips * 100:.2f}%')"
      ],
      "metadata": {
        "id": "TuOIIoVy2AC3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "febfab76-4a92-4e9c-f8b9-a2efc5f594a7"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of videos: 887\n",
            "Progress: 100.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if all embeddings were successfully loaded.\n",
        "if len(not_found_embeddings) > 0:\n",
        "  print(f' WARNING: Failed to find embeddings for {len(set(([item[0] for item in not_found_embeddings])))} videos.')\n",
        "\n",
        "else:\n",
        "  print('Success! All embeddings read.')"
      ],
      "metadata": {
        "id": "UbT6MVGK7vbX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65d16fdb-aabe-4e9e-eb9e-82f0e071657b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Success! All embeddings read.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if all embeddings have the required input size and pad the ones which do not.\n",
        "X_train_original = X_train.copy()\n",
        "X_test_original = X_test.copy()\n",
        "y_train_original = y_train.copy()\n",
        "y_test_original = y_test.copy()\n",
        "video_names_train_original = video_names_train.copy()\n",
        "video_names_test_original = video_names_test.copy()\n",
        "\n",
        "X_train = []\n",
        "X_test = []\n",
        "y_train = []\n",
        "y_test = []\n",
        "video_names_train = []\n",
        "video_names_test = []\n",
        "\n",
        "padded_train = 0\n",
        "padded_test = 0\n",
        "\n",
        "for i, array in enumerate(X_train_original):\n",
        "  if len(array) == input_size:\n",
        "    X_train.append(array)\n",
        "\n",
        "  elif len(array) < input_size:\n",
        "    difference = input_size - len(array)\n",
        "\n",
        "    padding_size_beginning = difference // 2 + ((difference % 2) * 1)\n",
        "    padding_size_end = difference // 2\n",
        "\n",
        "    padded_array = np.pad(array, (padding_size_beginning, padding_size_end), \\\n",
        "                          mode='constant', constant_values=(0, 0))\n",
        "\n",
        "    X_train.append(padded_array)\n",
        "\n",
        "    padded_train += 1\n",
        "\n",
        "  y_train.append(y_train_original[i])\n",
        "  video_names_train.append(video_names_train_original[i])\n",
        "\n",
        "for i, array in enumerate(X_test_original):\n",
        "  if len(array) == input_size:\n",
        "    X_test.append(array)\n",
        "\n",
        "  elif len(array) < input_size:\n",
        "    difference = input_size - len(array)\n",
        "\n",
        "    padding_size_beginning = difference // 2 + ((difference % 2) * 1)\n",
        "    padding_size_end = difference // 2\n",
        "\n",
        "    padded_array = np.pad(array, (padding_size_beginning, padding_size_end), \\\n",
        "                           mode='constant', constant_values=(0, 0))\n",
        "\n",
        "    X_test.append(padded_array)\n",
        "\n",
        "    padded_test += 1\n",
        "\n",
        "  y_test.append(y_test_original[i])\n",
        "  video_names_test.append(video_names_test_original[i])"
      ],
      "metadata": {
        "id": "X9M-a9FjSTGu"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if any embeddings had to be removed.\n",
        "print(f'{padded_train} embeddings were padded in training set due to incorrect embedding size.')\n",
        "print(f'{padded_test} embeddings were padded in test set due to incorrect embedding size.')"
      ],
      "metadata": {
        "id": "_XFITK_OUDT3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3bcfbb95-497e-46d7-9d0a-1e0b9a7730cc"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5 embeddings were padded in training set due to incorrect embedding size.\n",
            "1 embeddings were padded in test set due to incorrect embedding size.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting to numpy.\n",
        "X_train = np.array(X_train)\n",
        "X_test = np.array(X_test)\n",
        "y_train = np.array(y_train)\n",
        "y_test = np.array(y_test)"
      ],
      "metadata": {
        "id": "0Rgq195x26Xe"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Handle class imbalance through class weights\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "all_labels = np.concatenate((y_train, y_test))\n",
        "\n",
        "class_weights = compute_class_weight('balanced', classes=np.unique(all_labels), y=all_labels)\n",
        "class_weights_dict = {i: class_weights[i] for i in range(len(class_weights))}\n",
        "\n",
        "print('Class weights:')\n",
        "print(class_weights_dict)"
      ],
      "metadata": {
        "id": "59L6EKye3CCY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b962d4df-ccb9-4c95-d4ed-8fc98171358c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class weights:\n",
            "{0: 0.5502481389578163, 1: 5.4753086419753085}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cross-validate model"
      ],
      "metadata": {
        "id": "R30ljSCIHvQL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## FUNCTIONS FOR MODEL CROSS-VALIDATION.\n",
        "\n",
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras.optimizers import Adam\n",
        "from sklearn.model_selection import StratifiedKFold, KFold\n",
        "from sklearn.metrics import recall_score, accuracy_score, f1_score, roc_auc_score\n",
        "from IPython.display import clear_output\n",
        "\n",
        "def create_shallow_network(input_dim, hidden_layers, dropout_rate, learning_rate):\n",
        "    model = Sequential()\n",
        "\n",
        "    # Dense input layer with ReLu\n",
        "    model.add(Dense(10, input_dim=input_dim, activation='relu'))\n",
        "\n",
        "    # Dense hidden layers with ReLu and dropout\n",
        "    for _ in range(hidden_layers):\n",
        "        model.add(Dense(128, activation='relu'))\n",
        "        model.add(Dropout(dropout_rate))\n",
        "\n",
        "    # Dense output layer with sigmoid activation\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    optimizer = Adam(learning_rate=learning_rate)\n",
        "\n",
        "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "def crossval_shallow_network(n_splits, input_dim, X, y, params_grid, \\\n",
        "                             val_score = 'accuracy', verbose=False):\n",
        "\n",
        "  kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "\n",
        "  # Set up best result tracker for grid search loop\n",
        "  best_score = 0\n",
        "\n",
        "  best_params = {}\n",
        "\n",
        "  # Calculate number of runs for progress monitoring.\n",
        "  total_runs = 1\n",
        "\n",
        "  for params in params_grid.values():\n",
        "    total_runs *= len(params)\n",
        "\n",
        "  total_runs *= n_splits\n",
        "\n",
        "  progress = 0\n",
        "\n",
        "  # Grid search loop\n",
        "  for hidden_layers in params_grid['layers']:\n",
        "      for dropout_rate in params_grid['dropout_rate']:\n",
        "          for learning_rate in params_grid['learning_rate']:\n",
        "\n",
        "            scores = {threshold : [] for threshold in params_grid['pos_threshold']}\n",
        "\n",
        "            for train_index, test_index in kf.split(X):\n",
        "                X_train, X_test = X[train_index], X[test_index]\n",
        "                y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "                # Create and fit the model\n",
        "                model = create_shallow_network(input_dim, hidden_layers, \\\n",
        "                                                dropout_rate, learning_rate)\n",
        "\n",
        "                model.fit(X_train, y_train, epochs=10, verbose=0, class_weight=class_weights_dict)\n",
        "\n",
        "                # Sigmoid output\n",
        "                y_pred_raw = model.predict(X_test)\n",
        "\n",
        "                # Predicted labels based on sigmoid output\n",
        "                for threshold in params_grid['pos_threshold']:\n",
        "                  y_pred = (y_pred_raw > threshold).astype(int).squeeze()\n",
        "\n",
        "                  # Calculate performance based on selected score\n",
        "                  if val_score == 'accuracy':\n",
        "                    score = accuracy_score(y_test, y_pred)\n",
        "\n",
        "                  elif val_score == 'recall':\n",
        "                    score = recall_score(y_test, y_pred)\n",
        "\n",
        "                  elif val_score == 'f1_score':\n",
        "                    score = f1_score(y_test, y_pred)\n",
        "\n",
        "                  elif val_score == 'roc_auc':\n",
        "                    score = roc_auc_score(y_test, y_pred_raw)\n",
        "\n",
        "                  else:\n",
        "                    print(f'ERROR: unexpected validation score {val_score}.')\n",
        "                    print('Select one of: accuracy, recall, f1_score')\n",
        "\n",
        "                  scores[threshold].append(score)\n",
        "\n",
        "                  progress += 1\n",
        "\n",
        "                  clear_output(wait=True)\n",
        "                  print(f'Cross-validation progress: {progress / total_runs * 100:.2f}%')\n",
        "\n",
        "                # Best average score across all folds\n",
        "                best_threshold = max(scores, key=lambda thr: np.mean(scores[thr]))\n",
        "                best_average_score = np.mean(scores[best_threshold])\n",
        "\n",
        "                # Check if current model settings beat the current best\n",
        "                if best_average_score > best_score:\n",
        "                    best_score = best_average_score\n",
        "                    best_params = {'hidden_layers': hidden_layers,\n",
        "                                  'dropout_rate': dropout_rate,\n",
        "                                  'learning_rate': learning_rate,\n",
        "                                  'pos_threshold': best_threshold}\n",
        "\n",
        "  # Print best parameters and their score\n",
        "  if verbose:\n",
        "    print(f\"\\nBest Score: {best_score:.4f}\")\n",
        "    print(\"Best Parameters:\", best_params)\n",
        "\n",
        "  return best_params"
      ],
      "metadata": {
        "id": "nvWC2RMgpMF6"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting up model dictionary for ease of reuse.\n",
        "# Only models which do not need cross-validation are included in this.\n",
        "# Models with cross-validation are handled on an individual basis.\n",
        "\n",
        "models_dict = {\n",
        "    'LogisticRegression' : LogisticRegression(max_iter=1000, class_weight=class_weights_dict),\n",
        "    'SVC-Linear' : SVC(class_weight='balanced', kernel='linear')\n",
        "    }\n",
        "\n",
        "# Models which do not need cross-validation.\n",
        "no_cross_val = list(models_dict.keys())"
      ],
      "metadata": {
        "id": "HKtjad033aFR"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Selecting model (with potential cross validation).\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# If model requires cross-validation.\n",
        "if clf_name not in no_cross_val:\n",
        "  if clf_name == 'ShallowNetwork':\n",
        "    best_params = crossval_shallow_network(cv_folds, input_size, X_train, \\\n",
        "                                           y_train, param_grid['ShallowNetwork'],\\\n",
        "                                           val_score = val_score, verbose=True)\n",
        "\n",
        "    model = create_shallow_network(input_size, best_params['hidden_layers'], \\\n",
        "                                 best_params['dropout_rate'], best_params['learning_rate'])\n",
        "\n",
        "# If model does not require cross-validation.\n",
        "else:\n",
        "  model = models_dict[clf_name]"
      ],
      "metadata": {
        "id": "NE1joIrK3I6_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae69cdbf-3066-43f6-c764-7267e28668fe"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross-validation progress: 100.00%\n",
            "\n",
            "Best Score: 0.9393\n",
            "Best Parameters: {'hidden_layers': 0, 'dropout_rate': 0.25, 'learning_rate': 0.0001, 'pos_threshold': 0.1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train model on the training set.\n",
        "model.fit(X_train, y_train, class_weight=class_weights_dict)"
      ],
      "metadata": {
        "id": "Nn0TwY8yIbhx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33f69f6e-e519-443b-b622-35cf698b9724"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "24/24 [==============================] - 1s 3ms/step - loss: 0.6227 - accuracy: 0.6063\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7945d12a8940>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions.\n",
        "y_pred_raw = model.predict(X_test)"
      ],
      "metadata": {
        "id": "tMTqwQYe4EC5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1172b84c-cdc6-4b68-9408-6440494e18f0"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4/4 [==============================] - 0s 2ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose best threshold in case of networks validated on ROC AUC.\n",
        "# Threshold is selected to maximize F1 score.\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "\n",
        "if clf_name == 'ShallowNetwork' and val_score == 'roc_auc':\n",
        "  precision, recall, thresholds = precision_recall_curve(y_test, y_pred_raw)\n",
        "\n",
        "  # Calculate F1 Scores for different thresholds\n",
        "  f1_scores = 2 * recall * precision / (recall + precision)\n",
        "\n",
        "  f1_scores = f1_scores[:-1]    # Drop last score as that corresponds to -inf threshold.\n",
        "\n",
        "  f1_scores_nonnull = f1_scores[~np.isnan(f1_scores)]\n",
        "\n",
        "  thresholds_nonnull = thresholds[~np.isnan(f1_scores)]\n",
        "\n",
        "  optimal_idx = np.argmax(f1_scores_nonnull)\n",
        "\n",
        "  pos_threshold = thresholds_nonnull[optimal_idx]\n",
        "\n",
        "  best_params['pos_threshold'] = pos_threshold\n",
        "\n",
        "  print('Optimal threshold:', pos_threshold)"
      ],
      "metadata": {
        "id": "CyCuYAz_8FO7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8da1bcc8-78a3-4563-c56a-53f22ea8aa0d"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal threshold: 0.94437116\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-21-25a1d6c49907>:9: RuntimeWarning: invalid value encountered in divide\n",
            "  f1_scores = 2 * recall * precision / (recall + precision)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "  # Saving best hyperparameters.\n",
        "  best_params_df = pd.DataFrame([best_params])\n",
        "\n",
        "  params_output_path = os.path.join(output_dir, 'Parameters')\n",
        "\n",
        "  os.makedirs(params_output_path, exist_ok=True)\n",
        "\n",
        "  params_csv_path = os.path.join(output_dir, f'Parameters/params_{dataset_name}_{frame_selection_method}_{embedding_method}_{image_size}_{clf_name}_{val_score.replace(\"_\", \"-\")}.csv')\n",
        "\n",
        "  best_params_df.to_csv(params_csv_path, index=False)"
      ],
      "metadata": {
        "id": "o4u-3kZUATgz"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Predictions and testing"
      ],
      "metadata": {
        "id": "yMXGX5HiBOcr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save predictions for each instance of the test set."
      ],
      "metadata": {
        "id": "HwjnevSxD7Mk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare DataFrame with video names and predictions.\n",
        "predictions_df = pd.DataFrame({\n",
        "    'video': video_names_test,\n",
        "    'prediction': y_pred_raw.squeeze(),\n",
        "    'label' : y_test\n",
        "})\n",
        "\n",
        "# Create predictions output directory.\n",
        "pred_output_path = os.path.join(output_dir, 'Predictions')\n",
        "os.makedirs(pred_output_path, exist_ok=True)\n",
        "\n",
        "# Save predictions.\n",
        "predictions_csv_path = os.path.join(pred_output_path, f'pred_{dataset_name}_{frame_selection_method}_{embedding_method}_{image_size}_{clf_name}_{val_score.replace(\"_\", \"-\")}.csv')\n",
        "predictions_df.to_csv(predictions_csv_path, index=False)"
      ],
      "metadata": {
        "id": "jSTHAae23wqp"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Copy prediction files to Drive.\n",
        "for file in os.listdir(f'{output_dir}/Predictions'):\n",
        "  source_file = os.path.join(f'{output_dir}/Predictions', file)\n",
        "\n",
        "  if file.startswith('pred'):   # Only consider prediction outputs.\n",
        "    drive_output_dir = f\"/content/drive/My Drive/UvA/M Thesis/Data/Results/Predictions\"\n",
        "\n",
        "    if not os.path.exists(drive_output_dir):\n",
        "      os.makedirs(drive_output_dir)\n",
        "\n",
        "    destination = f'{drive_output_dir}/{file}'\n",
        "\n",
        "    # Overwrite existing file.\n",
        "    if os.path.exists(destination):\n",
        "      os.remove(destination)\n",
        "\n",
        "    shutil.copy(source_file, destination)\n",
        "\n",
        "for file in os.listdir(f'{output_dir}/Parameters'):\n",
        "  source_file = os.path.join(f'{output_dir}/Parameters', file)\n",
        "\n",
        "  if file.startswith('params'):   # Only consider parameter outputs.\n",
        "    drive_output_dir = f\"/content/drive/My Drive/UvA/M Thesis/Data/Results/Parameters\"\n",
        "\n",
        "    if not os.path.exists(drive_output_dir):\n",
        "      os.makedirs(drive_output_dir)\n",
        "\n",
        "    destination = f'{drive_output_dir}/{file}'\n",
        "\n",
        "    # Overwrite existing file.\n",
        "    if os.path.exists(destination):\n",
        "      os.remove(destination)\n",
        "\n",
        "    shutil.copy(source_file, destination)"
      ],
      "metadata": {
        "id": "5ABTeEmc9UCw"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classification report on the complete test set"
      ],
      "metadata": {
        "id": "B6eZ4Eg9EKAQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Turn sigmoid predictions into binary ones in case of a network.\n",
        "if clf_name == 'ShallowNetwork':\n",
        "  y_pred = (y_pred_raw > best_params['pos_threshold']).astype(int).squeeze()\n",
        "\n",
        "else:\n",
        "  y_pred = y_pred_raw"
      ],
      "metadata": {
        "id": "pKqRVkms8oy9"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Producing classification report.\n",
        "report = classification_report(y_test, y_pred, target_names=['No attack', 'Attack'])\n",
        "\n",
        "print(report)"
      ],
      "metadata": {
        "id": "R_4E06J24Gm7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d158800-7c6b-48eb-8acf-b55a92992632"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "   No attack       0.96      0.93      0.95       118\n",
            "      Attack       0.27      0.43      0.33         7\n",
            "\n",
            "    accuracy                           0.90       125\n",
            "   macro avg       0.62      0.68      0.64       125\n",
            "weighted avg       0.93      0.90      0.91       125\n",
            "\n"
          ]
        }
      ]
    }
  ]
}